{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from model import FairPFNModel\n",
    "from data_generator import DataGenerator\n",
    "from datasets import save_generated_data, SyntheticDataset\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = Path(\"data\", \"pre_generated_data.parquet\")\n",
    "if not os.path.exists(DATASET_FILE):\n",
    "    raise FileNotFoundError(f\"File {DATASET_FILE} does not exist.\")\n",
    "df = pd.read_parquet(DATASET_FILE)\n",
    "print(f\"Loaded {len(df)} samples from {DATASET_FILE}.\")\n",
    "print(\"Number of biased features:\", df.shape[1] - 3)  # Exclude label column\n",
    "print(\"Unique biased labels: \", df.iloc[:, -2].value_counts().to_dict())\n",
    "print(\"Unique fair labels: \", df.iloc[:, -1].value_counts().to_dict())\n",
    "print(f\"Number of unique biased features: {len(set(df.iloc[:, 1:-2].values.flatten()))}, number of total biased features: {len(df.iloc[:, 1:-2].values.flatten())}\")\n",
    "print(\"Biggest biased feature value:\", df.iloc[:, 1:-2].values.flatten().max())\n",
    "print(\"Smallest biased feature value:\", df.iloc[:, 1:-2].values.flatten().min())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44042f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_different = df[(df.iloc[:, -2].astype(int) != df.iloc[:, -1])]\n",
    "\n",
    "\n",
    "# sample five random rows from the filtered DataFrame\n",
    "sampled_rows = labels_different.sample(n=5)\n",
    "print(\"Sampled rows with different biased and fair labels:\")\n",
    "print(sampled_rows.to_string(index=False))\n",
    "print(sampled_rows.to_latex(index=False)) \n",
    "# rows = df[(df.iloc[:, -2].astype(int) != df.iloc[:, -1]) & (df.iloc[:, 1].map(df.iloc[:, 1].value_counts()) == 2)]\n",
    "# print((df.iloc[:, 0].map(df.iloc[:, 0].value_counts()) > 1).sum(), \"rows with unique element count in f0 > 1\")\n",
    "# print(\"Rows with unique element count in f0 > 1 and f4 not equal to y_fair:\")\n",
    "# print(rows.head(5).to_string(index=False))\n",
    "# print(df.iloc[15:20, :].to_string(index=False))\n",
    "# print(df.describe().to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7958ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table\n",
    "latex_table = r\"\"\"\\begin{table}[h]\n",
    "\\centering\n",
    "\\begin{tabular}{ll}\n",
    "\\hline\n",
    "\\textbf{Statistic} & \\textbf{Value} \\\\\n",
    "\\hline\n",
    "Total samples & \"\"\" + f\"{df.shape[0]}\" + r\"\"\" \\\\\n",
    "Number of biased features & \"\"\" + f\"{df.shape[1] - 3}\" + r\"\"\" \\\\\n",
    "Unique biased labels & \"\"\" + f\"{df.iloc[:, -2].value_counts().to_dict()}\" + r\"\"\" \\\\\n",
    "Unique fair labels & \"\"\" + f\"{df.iloc[:, -1].value_counts().to_dict()}\" + r\"\"\" \\\\\n",
    "Number of unique biased features & \"\"\" + f\"{len(set(df.iloc[:, 1:-2].values.flatten()))}\" + r\"\"\" \\\\\n",
    "Total biased feature values & \"\"\" + f\"{len(df.iloc[:, 1:-2].values.flatten())}\" + r\"\"\" \\\\\n",
    "Max biased feature value & \"\"\" + f\"{df.iloc[:, 1:-2].values.flatten().max()}\" + r\"\"\" \\\\\n",
    "Min biased feature value & \"\"\" + f\"{df.iloc[:, 1:-2].values.flatten().min()}\" + r\"\"\" \\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Summary statistics of synthetic dataset}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "# Output the LaTeX table\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f31d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of biased features\n",
    "import matplotlib.pyplot as plt\n",
    "df.iloc[:, 1:-2].hist(bins=30, figsize=(15, 10))\n",
    "plt.suptitle(\"Distribution of Biased Features\")\n",
    "plt.xlabel(\"Feature Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the distribution of fair labels\n",
    "df.iloc[:, -1].value_counts().plot(kind='bar', figsize=(10, 5), title='Distribution of Fair Labels')\n",
    "plt.xlabel('Fair Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plotting the distribution of biased labels\n",
    "df.iloc[:, -2].value_counts().plot(kind='bar', figsize=(10, 5), title='Distribution of Biased Labels')\n",
    "plt.xlabel('Biased Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 1000\n",
    "mu, sigma = 0, 1\n",
    "w_A = 2.0\n",
    "w_Xb = 1.5\n",
    "\n",
    "# Generate noise\n",
    "eps_Xb = np.random.normal(mu, sigma, n)\n",
    "eps_Y = np.random.normal(mu, sigma, n)\n",
    "\n",
    "# Generate protected attribute A\n",
    "A = np.random.randint(0, 2, size=n)  # A âˆˆ {0,1}\n",
    "\n",
    "# Generate features and outcome (observational)\n",
    "Xb = w_A * A**2 + eps_Xb\n",
    "Y_cont = w_Xb * Xb**2 + eps_Y\n",
    "Y_threshold = np.median(Y_cont)\n",
    "Y = (Y_cont >= Y_threshold).astype(int)\n",
    "\n",
    "# Observational data\n",
    "observational_df = pd.DataFrame({'A': A, 'Xb': Xb, 'Y': Y})\n",
    "\n",
    "# --- Counterfactual Generation ---\n",
    "\n",
    "def generate_counterfactual(A_new, eps_Xb, eps_Y):\n",
    "    Xb_cf = w_A * A_new**2 + eps_Xb\n",
    "    Y_cont_cf = w_Xb * Xb_cf**2 + eps_Y\n",
    "    Y_cf = (Y_cont_cf >= Y_threshold).astype(int)\n",
    "    return Xb_cf, Y_cf\n",
    "\n",
    "# Counterfactuals for do(A=0) and do(A=1)\n",
    "Xb_do0, Y_do0 = generate_counterfactual(np.zeros(n), eps_Xb, eps_Y)\n",
    "Xb_do1, Y_do1 = generate_counterfactual(np.ones(n), eps_Xb, eps_Y)\n",
    "\n",
    "counterfactual_df = pd.DataFrame({\n",
    "    'A_obs': A,\n",
    "    'Y_obs': Y,\n",
    "    'Y_do0': Y_do0,\n",
    "    'Y_do1': Y_do1\n",
    "})\n",
    "\n",
    "average_treatment_effect = np.mean(Y_do1) - np.mean(Y_do0)\n",
    "print(f\"Average Treatment Effect (ATE): {average_treatment_effect}\")\n",
    "print(\"Percentage of individuals with Y_do1 == Y_do0:\", np.mean(Y_do1 == Y_do0) * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d558056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(Xb, A, Y, split_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Prepares data for the model by splitting into training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        Xb (np.ndarray): Feature array.\n",
    "        A (np.ndarray): Protected attribute.\n",
    "        Y (np.ndarray): Labels.\n",
    "        split_ratio (float): Ratio to split the data into training and test sets.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Training and test sets as DataFrames.\n",
    "    \"\"\"\n",
    "    Xb_tensor = torch.tensor(Xb, dtype=torch.float32).unsqueeze(1)\n",
    "    A_tensor = torch.tensor(A, dtype=torch.float32).unsqueeze(1)\n",
    "    Y_tensor = torch.tensor(Y, dtype=torch.float32).unsqueeze(1)\n",
    "    #concat A_tensor and Xb_tensor into X_bias\n",
    "    X_biased = torch.cat((A_tensor, Xb_tensor), dim=1)\n",
    "    split = int(0.75* len(Xb_tensor))\n",
    "    forward_kwargs = dict(\n",
    "                    train_x = Xb_tensor[:split, :].unsqueeze(1),\n",
    "                    train_y = Y_tensor[:split, :].unsqueeze(1),\n",
    "                    test_x = Xb_tensor[split:, :].unsqueeze(1),\n",
    "                    categorical_inds=None,\n",
    "                )\n",
    "    return forward_kwargs, Y_tensor[split:, :].squeeze(), len(torch.unique(Y_tensor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccace8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the FairPFN model\n",
    "model = FairPFNModel(device='cpu')\n",
    "model.load_model('models/fairpfn_model_epoch_100.pth')\n",
    "print(\"Unique count values in Y: \", np.unique(Y, return_counts=True))\n",
    "# Predict using the FairPFN model\n",
    "split_ratio = 0.75\n",
    "forward_kwargs, test_labels, num_classes = prepare_data_for_model(\n",
    "    Xb=Xb,\n",
    "    A=A,\n",
    "    Y=Y,\n",
    "    split_ratio=split_ratio\n",
    ")\n",
    "\n",
    "pred_fair_logits = model(**forward_kwargs)\n",
    "pred_fair_logits = pred_fair_logits[:, :, :num_classes]\n",
    "pred_fair_logits = pred_fair_logits.reshape(-1, pred_fair_logits.shape[-1])\n",
    "print(\"Predicted labels:\", pred_fair_logits.shape)\n",
    "accuracy = (pred_fair_logits.argmax(dim=1) == test_labels).float().mean().item()\n",
    "print(f\"FairPFN Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in A and the counts: \", counterfactual_df['A_obs'].value_counts().to_dict())\n",
    "print(\"Unique values in Y_obs and the counts: \", counterfactual_df['Y_obs'].value_counts().to_dict())\n",
    "print(\"Unique values in Y_do0 and the counts: \", counterfactual_df['Y_do0'].value_counts().to_dict())\n",
    "print(\"Unique values in Y_do1 and the counts: \", counterfactual_df['Y_do1'].value_counts().to_dict())\n",
    "print(observational_df.head())\n",
    "print(counterfactual_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(\n",
    "    U=16,  # Number of exogenous variables\n",
    "    H=3,   # MLP depth\n",
    "    M=16,  # Number of features\n",
    "    N=10000,  # Number of samples\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "dataset_biased, y_fair = data_generator.generate_dataset()\n",
    "dataset_biased_2, y_fair_2 = data_generator.generate_dataset()\n",
    "\n",
    "# check if the two torch tensors are equal\n",
    "if torch.equal(dataset_biased, dataset_biased_2) and torch.equal(y_fair, y_fair_2):\n",
    "    print(\"The two datasets are equal.\")\n",
    "    print(\"The two y_fair tensors are equal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef62553",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_A0 = data_generator.do_A(A = 0)\n",
    "do_A1 = data_generator.do_A(A = 1)\n",
    "\n",
    "if torch.equal(do_A0, do_A1):\n",
    "    print(\"The two do_A tensors are equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique count values in do_A0:\", torch.unique(do_A0[:, -1], return_counts=True))\n",
    "base_causal_effect = do_A0[:, -1].float().mean() - do_A1[:, -1].float().mean()\n",
    "print(f\"Base causal effect: {base_causal_effect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.75 * len(do_A0))\n",
    "\n",
    "incontext_biased_features = do_A0[:split, :-1].unsqueeze(1)\n",
    "incontext_biased_labels = do_A0[:split, -1].unsqueeze(1)\n",
    "val_biased_features = do_A0[split:, :-1].unsqueeze(1)\n",
    "num_classes = len(torch.unique(incontext_biased_labels))\n",
    "forward_kwargs = dict(\n",
    "    train_x=incontext_biased_features,\n",
    "    train_y=incontext_biased_labels,\n",
    "    test_x=val_biased_features,\n",
    "    categorical_inds=None,\n",
    ")\n",
    "pred_fair_logits = model(**forward_kwargs)\n",
    "pred_fair_logits = pred_fair_logits[:, :, :num_classes]\n",
    "pred_fair_logits = pred_fair_logits.reshape(-1, pred_fair_logits.shape[-1])\n",
    "predicted_labels_A0 = pred_fair_logits.argmax(dim=1)\n",
    "\n",
    "incontext_biased_features = do_A1[:split, :-1].unsqueeze(1)\n",
    "incontext_biased_labels = do_A1[:split, -1].unsqueeze(1)\n",
    "val_biased_features = do_A1[split:, :-1].unsqueeze(1)\n",
    "forward_kwargs = dict(\n",
    "    train_x=incontext_biased_features,\n",
    "    train_y=incontext_biased_labels,\n",
    "    test_x=val_biased_features,\n",
    "    categorical_inds=None,\n",
    ")\n",
    "pred_fair_logits = model(**forward_kwargs)\n",
    "pred_fair_logits = pred_fair_logits[:, :, :num_classes]\n",
    "pred_fair_logits = pred_fair_logits.reshape(-1, pred_fair_logits.shape[-1])\n",
    "predicted_labels_A1 = pred_fair_logits.argmax(dim=1)\n",
    "average_treatment_effect = predicted_labels_A1.float().mean() - predicted_labels_A0.float().mean()\n",
    "print(f\"Average Treatment Effect (ATE) from model predictions: {average_treatment_effect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c2d4c",
   "metadata": {},
   "source": [
    "## Generate datasets with certain base_causal_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68585a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_counter = 0\n",
    "while datasets_counter < 5:\n",
    "    data_generator = DataGenerator(\n",
    "        U=16,  # Number of exogenous variables\n",
    "        H=3,   # MLP depth\n",
    "        M=16,  # Number of features\n",
    "        N=256,  # Number of samples\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    dataset_biased, y_fair = data_generator.generate_dataset()\n",
    "    do_A0 = data_generator.do_A(A=0)\n",
    "    do_A1 = data_generator.do_A(A=1)\n",
    "    base_causal_effect = do_A0[:, -1].float().mean() - do_A1[:, -1].float().mean()\n",
    "    if abs(base_causal_effect) > 0.3:\n",
    "        print(f\"Generated dataset with base causal effect: {base_causal_effect}\")\n",
    "        # Save the dataset to a file\n",
    "        save_generated_data(\n",
    "            Dbias=dataset_biased,\n",
    "            y_fair=y_fair,\n",
    "            filename=f\"data/generated_data_observational.parquet\"\n",
    "        )\n",
    "        save_generated_data(\n",
    "            Dbias=do_A0,\n",
    "            y_fair=y_fair,\n",
    "            filename=f\"data/generated_data_do_A0.parquet\"\n",
    "        )\n",
    "        save_generated_data(\n",
    "            Dbias=do_A1,\n",
    "            y_fair=y_fair,\n",
    "            filename=f\"data/generated_data_do_A1.parquet\"\n",
    "        )\n",
    "        datasets_counter += 1\n",
    "print(f\"Generated {datasets_counter} datasets with base causal effect greater than 0.3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a8d83",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = Path(\"data\")\n",
    "# DATASET_OBSERVATIONAL = Path(DATA_DIR, \"generated_data_observational.parquet\")\n",
    "# if not os.path.exists(DATASET_OBSERVATIONAL):\n",
    "#     raise FileNotFoundError(f\"File {DATASET_OBSERVATIONAL} does not exist.\")\n",
    "# df_observational = pd.read_parquet(DATASET_OBSERVATIONAL)\n",
    "# print(f\"Loaded {len(df_observational)} samples from {DATASET_OBSERVATIONAL}.\")\n",
    "\n",
    "# DATASET_DO_A0 = Path(DATA_DIR, \"generated_data_do_A0.parquet\")\n",
    "# if not os.path.exists(DATASET_DO_A0):\n",
    "#     raise FileNotFoundError(f\"File {DATASET_DO_A0} does not exist.\")\n",
    "# df_do_A0 = pd.read_parquet(DATASET_DO_A0)\n",
    "# print(f\"Loaded {len(df_do_A0)} samples from {DATASET_DO_A0}.\")\n",
    "\n",
    "# DATASET_DO_A1 = Path(DATA_DIR, \"generated_data_do_A1.parquet\")\n",
    "# if not os.path.exists(DATASET_DO_A1):\n",
    "#     raise FileNotFoundError(f\"File {DATASET_DO_A1} does not exist.\")\n",
    "# df_do_A1 = pd.read_parquet(DATASET_DO_A1)\n",
    "# print(f\"Loaded {len(df_do_A1)} samples from {DATASET_DO_A1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATASET_DO_A0 = Path(DATA_DIR, \"generated_data_do_A0.parquet\")\n",
    "DATASET_DO_A1 = Path(DATA_DIR, \"generated_data_do_A1.parquet\")\n",
    "\n",
    "datasets_do_A0 = SyntheticDataset(\n",
    "    filename=DATASET_DO_A0,\n",
    ")\n",
    "datasets_do_A1 = SyntheticDataset(\n",
    "    filename=DATASET_DO_A1,\n",
    ")\n",
    "\n",
    "expectations = {'A0': [], 'A1': []}\n",
    "for dataset, _ in datasets_do_A0:\n",
    "    expectation = dataset[:, -1].float().mean()\n",
    "    expectations['A0'].append(expectation)\n",
    "\n",
    "for dataset, _ in datasets_do_A1:\n",
    "    expectation = dataset[:, -1].float().mean()\n",
    "    expectations['A1'].append(expectation)\n",
    "\n",
    "average_treatment_effect_base = np.array(expectations['A0']) - np.array(expectations['A1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base Average Treatment Effect (ATE): {average_treatment_effect_base}\")\n",
    "# save the generated ate to a file\n",
    "output_file = Path(DATA_DIR, \"average_treatment_effect_base.npy\")\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "np.save(output_file, average_treatment_effect_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd99e16",
   "metadata": {},
   "source": [
    "#### FairPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a07efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FairPFNModel()\n",
    "model.load_model('models/fairpfn_model_epoch_100.pth', eval_mode=True)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016faa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_do_A0 = SyntheticDataset(\n",
    "    filename=DATASET_DO_A0,\n",
    ")\n",
    "datasets_do_A1 = SyntheticDataset(\n",
    "    filename=DATASET_DO_A1,\n",
    ")\n",
    "\n",
    "expectations = {'A0': [], 'A1': []}\n",
    "for dataset, _ in datasets_do_A0:\n",
    "    split = int(0.75 * len(dataset))\n",
    "    forward_kwargs = dict(\n",
    "        train_x=dataset[:split, :-1].unsqueeze(1),\n",
    "        train_y=dataset[:split, -1].unsqueeze(1),\n",
    "        test_x=dataset[split:, :-1].unsqueeze(1),\n",
    "        categorical_inds=None,\n",
    "    )\n",
    "    num_classes = len(torch.unique(dataset[:split, -1].unsqueeze(1)))\n",
    "    pred_fair_logits = model(**forward_kwargs)\n",
    "    pred_fair_logits = pred_fair_logits[:, :, :num_classes]\n",
    "    pred_fair_logits = pred_fair_logits.reshape(-1, pred_fair_logits.shape[-1])\n",
    "    predicted_labels_do_A0 = pred_fair_logits.argmax(dim=1)\n",
    "    expectations['A0'].append(predicted_labels_do_A0.float().mean().item())\n",
    "\n",
    "for dataset, _ in datasets_do_A1:\n",
    "    split = int(0.75 * len(dataset))\n",
    "    forward_kwargs = dict(\n",
    "        train_x=dataset[:split, :-1].unsqueeze(1),\n",
    "        train_y=dataset[:split, -1].unsqueeze(1),\n",
    "        test_x=dataset[split:, :-1].unsqueeze(1),\n",
    "        categorical_inds=None,\n",
    "    )\n",
    "    num_classes = len(torch.unique(dataset[:split, -1].unsqueeze(1)))\n",
    "    pred_fair_logits = model(**forward_kwargs)\n",
    "    pred_fair_logits = pred_fair_logits[:, :, :num_classes]\n",
    "    pred_fair_logits = pred_fair_logits.reshape(-1, pred_fair_logits.shape[-1])\n",
    "    predicted_labels_do_A1 = pred_fair_logits.argmax(dim=1)\n",
    "    expectations['A1'].append(predicted_labels_do_A1.float().mean().item())\n",
    "\n",
    "average_treatment_effect_fairpfn = np.array(expectations['A0']) - np.array(expectations['A1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60140e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Treatment Effect (ATE) of FairPFN: {average_treatment_effect_fairpfn}\")\n",
    "print(\"Number of ate < 0.3:\", np.sum(np.abs(average_treatment_effect_fairpfn) < 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ccbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the generated average treatment effect to a file\n",
    "output_file = Path(DATA_DIR, \"average_treatment_effect_fairpfn.npy\")\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "np.save(output_file, average_treatment_effect_fairpfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857c9b4",
   "metadata": {},
   "source": [
    "### TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FairPFNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_do_A0 = SyntheticDataset(\n",
    "    filename=DATASET_DO_A0,\n",
    ")\n",
    "datasets_do_A1 = SyntheticDataset(\n",
    "    filename=DATASET_DO_A1,\n",
    ")\n",
    "\n",
    "expectations = {'A0': [], 'A1': []}\n",
    "for dataset, _ in datasets_do_A0:\n",
    "    split = int(0.75 * len(dataset))\n",
    "    forward_kwargs = dict(\n",
    "        train_x=dataset[:split, :-1].unsqueeze(1),\n",
    "        train_y=dataset[:split, -1].unsqueeze(1),\n",
    "        test_x=dataset[split:, :-1].unsqueeze(1),\n",
    "        categorical_inds=None,\n",
    "    )\n",
    "    num_classes = len(torch.unique(dataset[:split, -1].unsqueeze(1)))\n",
    "    pred_fair_logits = model(**forward_kwargs)\n",
    "    pred_fair_logits = pred_fair_logits[:, :, :num_classes]\n",
    "    pred_fair_logits = pred_fair_logits.reshape(-1, pred_fair_logits.shape[-1])\n",
    "    predicted_labels_do_A0 = pred_fair_logits.argmax(dim=1)\n",
    "    expectations['A0'].append(predicted_labels_do_A0.float().mean().item())\n",
    "\n",
    "for dataset, _ in datasets_do_A1:\n",
    "    split = int(0.75 * len(dataset))\n",
    "    forward_kwargs = dict(\n",
    "        train_x=dataset[:split, :-1].unsqueeze(1),\n",
    "        train_y=dataset[:split, -1].unsqueeze(1),\n",
    "        test_x=dataset[split:, :-1].unsqueeze(1),\n",
    "        categorical_inds=None,\n",
    "    )\n",
    "    num_classes = len(torch.unique(dataset[:split, -1].unsqueeze(1)))\n",
    "    pred_fair_logits = model(**forward_kwargs)\n",
    "    pred_fair_logits = pred_fair_logits[:, :, :num_classes]\n",
    "    pred_fair_logits = pred_fair_logits.reshape(-1, pred_fair_logits.shape[-1])\n",
    "    predicted_labels_do_A1 = pred_fair_logits.argmax(dim=1)\n",
    "    expectations['A1'].append(predicted_labels_do_A1.float().mean().item())\n",
    "\n",
    "average_treatment_effect_tabpfn = np.array(expectations['A0']) - np.array(expectations['A1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Treatment Effect (ATE) of TabPFN: {average_treatment_effect_tabpfn}\")\n",
    "print(\"Number of ate < 0.3:\", np.sum(np.abs(average_treatment_effect_tabpfn) < 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the generated ate to a file\n",
    "output_file = Path(DATA_DIR, \"average_treatment_effect_tabpfn.npy\")\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "np.save(output_file, average_treatment_effect_tabpfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9b4e7",
   "metadata": {},
   "source": [
    "# Load the generated ATEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "average_treatment_effect_base = np.load(Path(DATA_DIR, \"average_treatment_effect_base.npy\"))\n",
    "average_treatment_effect_fairpfn = np.load(Path(DATA_DIR, \"average_treatment_effect_fairpfn.npy\"))\n",
    "average_treatment_effect_tabpfn = np.load(Path(DATA_DIR, \"average_treatment_effect_tabpfn.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4caa6",
   "metadata": {},
   "source": [
    "### Box and whiskers plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06264d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box and whiskers plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([average_treatment_effect_base, average_treatment_effect_fairpfn, average_treatment_effect_tabpfn],\n",
    "            labels=['Base ATE', 'FairPFN ATE', 'TabPFN ATE'])\n",
    "plt.title('Box and Whiskers Plot of Average Treatment Effects')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Treatment Effect (ATE)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b6ca2",
   "metadata": {},
   "source": [
    "# KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel density estimation (KDE) plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.kdeplot(average_treatment_effect_base, label='Base ATE', fill=True, alpha=0.5)\n",
    "sns.kdeplot(average_treatment_effect_fairpfn, label='FairPFN ATE', fill=True, alpha=0.5)\n",
    "sns.kdeplot(average_treatment_effect_tabpfn, label='TabPFN ATE', fill=True, alpha=0.5)\n",
    "plt.title('Kernel Density Estimation of Average Treatment Effects')\n",
    "plt.xlabel('Average Treatment Effect (ATE)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair-pfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
